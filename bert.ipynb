{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e1c04ae-a3a2-4b05-8d3c-a724e7673625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cf14902-c183-4ce8-bd2d-36d0ad23029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionEmbed(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_model, max_len=512) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding is a matrix of size (max_len, dim_model)\n",
    "        # for each possible position i, j contains the sinusoid of frequency i / 10000^(2j/dim_model)\n",
    "        pe = torch.zeros(max_len, dim_model)\n",
    "        pe.requires_grad = False\n",
    "        \n",
    "        # create a 2D tensor with the position indices\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = (torch.arange(0, dim_model, 2, dtype=torch.float) * -(math.log(10000.0) / dim_model)).exp()\n",
    "\n",
    "        # for each 2 entries, starting at 0, we get a sin and cos activation\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the position embeddings for all tokens up to the current position idx\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=512, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        embedding_dim = self.token.embedding_dim\n",
    "        self.position = PositionEmbed(embedding_dim, embed_size)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence)\n",
    "        x = x + self.position(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Compute a single attention head\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # matrix multiplication of query and key, then scaled by the square root of the dimension of the query\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attn_heads, hidden, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        assert hidden % attn_heads == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = hidden // attn_heads\n",
    "        self.h = attn_heads\n",
    "\n",
    "        # linear layers for query, key and value\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(hidden, hidden) for _ in range(3)])\n",
    "        # final linear layer for output\n",
    "        self.output_linear = nn.Linear(hidden, hidden)\n",
    "        \n",
    "        # attention - performed per batch of queries\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # linear projection from hidden to d_k * h\n",
    "        # i.e. for each linear layer, we get the query, key and value\n",
    "        # these represent the linear layer for each head\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # compute attention for all heads in a batch\n",
    "        x, attention = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # concatenate all heads\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        # apply final linear layer\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, dropout) -> None:\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Feed forward layer, with dropout and GELU activation\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, feed_forward_hidden, dropout=0.1) -> None:\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(hidden, feed_forward_hidden)\n",
    "        self.w_2 = nn.Linear(feed_forward_hidden, hidden)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # gelu is the same as RELU with a slight dip before 0\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(attn_heads, hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(hidden, feed_forward_hidden, dropout)\n",
    "        self.input_sublayer = SublayerConnection(hidden, dropout)\n",
    "        self.output_sublayer = SublayerConnection(hidden, dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention(_x, _x, _x, mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads \n",
    "\n",
    "        self.feed_forward_hidden = hidden * 4 # 4 is hyperparameter\n",
    "\n",
    "        self.embedding = BERTEmbedding(vocab_size, hidden, dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            [TransformerBlock(hidden, attn_heads, dropout) for _ in range(n_layers)]\n",
    "        ])\n",
    "\n",
    "        # masked LM\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # attention mask for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # get the embedding for the input sequence\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, mask)\n",
    "\n",
    "        # masked LM\n",
    "        x = self.softmax(self.linear(x))\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
