{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c04ae-a3a2-4b05-8d3c-a724e7673625",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers datasets -q\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cf14902-c183-4ce8-bd2d-36d0ad23029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PositionEmbed(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim_model, max_len=512) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # embedding is a matrix of size (max_len, dim_model)\n",
    "        # for each possible position i, j contains the sinusoid of frequency i / 10000^(2j/dim_model)\n",
    "        pe = torch.zeros(max_len, dim_model)\n",
    "        pe.requires_grad = False\n",
    "        \n",
    "        # create a 2D tensor with the position indices\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = (torch.arange(0, dim_model, 2, dtype=torch.float) * -(math.log(10000.0) / dim_model)).exp()\n",
    "\n",
    "        # for each 2 entries, starting at 0, we get a sin and cos activation\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get the position embeddings for all tokens up to the current position idx\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=512, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        print(\"Embedding size: \", vocab_size, embed_size, dropout)\n",
    "        \n",
    "        self.token = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        embedding_dim = self.token.embedding_dim\n",
    "        print(\"Embedding dim: \", embedding_dim)\n",
    "        self.position = PositionEmbed(dim_model=embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        x = self.token(sequence)\n",
    "        print(\"X shape: \", x.shape, self.position(x).shape)\n",
    "        x = x + self.position(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "# Compute a single attention head\n",
    "class Attention(nn.Module):\n",
    "    \n",
    "    # matrix multiplication of query and key, then scaled by the square root of the dimension of the query\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.size(-1))\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, attn_heads, hidden, dropout=0.1) -> None:\n",
    "        super().__init__()\n",
    "        assert hidden % attn_heads == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = hidden // attn_heads\n",
    "        self.h = attn_heads\n",
    "\n",
    "        # linear layers for query, key and value\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(hidden, hidden) for _ in range(3)])\n",
    "        # final linear layer for output\n",
    "        self.output_linear = nn.Linear(hidden, hidden)\n",
    "        \n",
    "        # attention - performed per batch of queries\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # linear projection from hidden to d_k * h\n",
    "        # i.e. for each linear layer, we get the query, key and value\n",
    "        # these represent the linear layer for each head\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2) for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # compute attention for all heads in a batch\n",
    "        x, attention = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # concatenate all heads\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        # apply final linear layer\n",
    "        return self.output_linear(x)\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, dropout) -> None:\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = nn.LayerNorm(hidden)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Feed forward layer, with dropout and GELU activation\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, feed_forward_hidden, dropout=0.1) -> None:\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(hidden, feed_forward_hidden)\n",
    "        self.w_2 = nn.Linear(feed_forward_hidden, hidden)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # gelu is the same as RELU with a slight dip before 0\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(attn_heads, hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(hidden, feed_forward_hidden, dropout)\n",
    "        self.input_sublayer = SublayerConnection(hidden, dropout)\n",
    "        self.output_sublayer = SublayerConnection(hidden, dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention(_x, _x, _x, mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads \n",
    "\n",
    "        self.feed_forward_hidden = hidden * 4 # 4 is hyperparameter\n",
    "\n",
    "        print(\"BERT model: \", vocab_size, hidden, n_layers, attn_heads, dropout)\n",
    "        self.embedding = BERTEmbedding(vocab_size, hidden, dropout)\n",
    "\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden*4, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "        # masked LM\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # attention mask for padded token\n",
    "        # torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # get the embedding for the input sequence\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer(x, mask)\n",
    "\n",
    "        # masked LM\n",
    "        x = self.softmax(self.linear(x))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration Gustavosta--Stable-Diffusion-Prompts-d22aeec0ba2a9fdb\n",
      "Found cached dataset parquet (C:/Users/coold/.cache/huggingface/datasets/Gustavosta___parquet/Gustavosta--Stable-Diffusion-Prompts-d22aeec0ba2a9fdb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6dd81b6e7748f0bcf29c5679e17b1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\coold\\.cache\\huggingface\\datasets\\Gustavosta___parquet\\Gustavosta--Stable-Diffusion-Prompts-d22aeec0ba2a9fdb\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-32927d571d57457b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\coold\\.cache\\huggingface\\datasets\\Gustavosta___parquet\\Gustavosta--Stable-Diffusion-Prompts-d22aeec0ba2a9fdb\\0.0.0\\2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec\\cache-e66740f71a08c0b8.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "dataset = load_dataset(\"Gustavosta/Stable-Diffusion-Prompts\")\n",
    "tokenizer: BertTokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "dataset = dataset.map(lambda x: tokenizer(x[\"Prompt\"], truncation=True, padding=\"max_length\", max_length=512), batched=True)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0bd2485c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "class ScheduledOptim():\n",
    "    '''A simple wrapper class for learning rate scheduling'''\n",
    "\n",
    "    def __init__(self, optimizer, d_model, n_warmup_steps):\n",
    "        self._optimizer = optimizer\n",
    "        self.n_warmup_steps = n_warmup_steps\n",
    "        self.n_current_steps = 0\n",
    "        self.init_lr = np.power(d_model, -0.5)\n",
    "\n",
    "    def step_and_update_lr(self):\n",
    "        \"Step with the inner optimizer\"\n",
    "        self._update_learning_rate()\n",
    "        self._optimizer.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"Zero out the gradients by the inner optimizer\"\n",
    "        self._optimizer.zero_grad()\n",
    "\n",
    "    def _get_lr_scale(self):\n",
    "        return np.min([\n",
    "            np.power(self.n_current_steps, -0.5),\n",
    "            np.power(self.n_warmup_steps, -1.5) * self.n_current_steps])\n",
    "\n",
    "    def _update_learning_rate(self):\n",
    "        ''' Learning rate scheduling per step '''\n",
    "\n",
    "        self.n_current_steps += 1\n",
    "        lr = self.init_lr * self._get_lr_scale()\n",
    "\n",
    "        for param_group in self._optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "\n",
    "class BERTTrainer:\n",
    "    \n",
    "    def __init__(self, bert: BERT, vocab_size: int,\n",
    "                train_dataloader: DataLoader, test_dataloader: DataLoader = None,\n",
    "                lr: float = 1e-4, betas=(0.9, 0.999), weight_decay: float = 0.01, warmup_steps=10000,\n",
    "                with_cuda: bool = True, cuda_devices=None, log_freq: int = 10):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which you want to train\n",
    "        :param vocab_size: total word vocab size\n",
    "        :param train_dataloader: train dataset data loader\n",
    "        :param test_dataloader: test dataset data loader [can be None]\n",
    "        :param lr: learning rate of optimizer\n",
    "        :param betas: Adam optimizer betas\n",
    "        :param weight_decay: Adam optimizer weight decay param\n",
    "        :param with_cuda: traning with cuda\n",
    "        :param log_freq: logging frequency of the batch iteration\n",
    "        \"\"\"\n",
    "\n",
    "        # Setup cuda device for BERT training, argument -c, --cuda should be true\n",
    "        cuda_condition = torch.cuda.is_available() and with_cuda\n",
    "        self.device = torch.device(\"cuda:0\" if cuda_condition else \"cpu\")\n",
    "        print(\"Device:\", self.device)\n",
    "        print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "\n",
    "        # This BERT model will be saved every epoch\n",
    "        # Initialize the BERT Language Model, with BERT model\n",
    "        self.model = bert.to(self.device)\n",
    "\n",
    "        # Distributed GPU training if CUDA can detect more than 1 GPU\n",
    "        if with_cuda and torch.cuda.device_count() > 1:\n",
    "            print(\"Using %d GPUS for BERT\" % torch.cuda.device_count())\n",
    "            self.model = nn.DataParallel(self.model, device_ids=cuda_devices)\n",
    "\n",
    "        # Setting the train and test data loader\n",
    "        self.train_data = train_dataloader\n",
    "        self.test_data = test_dataloader\n",
    "\n",
    "        # Setting the Adam optimizer with hyper-param\n",
    "        self.optim = Adam(self.model.parameters(), lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "        self.optim_schedule = ScheduledOptim(self.optim, self.model.hidden, n_warmup_steps=warmup_steps)\n",
    "\n",
    "        # Using Negative Log Likelihood Loss function for predicting the masked_token\n",
    "        self.criterion = nn.NLLLoss(ignore_index=0)\n",
    "\n",
    "        self.log_freq = log_freq\n",
    "\n",
    "        print(\"Total Parameters:\", sum([p.nelement() for p in self.model.parameters()]))\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.iteration(epoch, self.train_data)\n",
    "\n",
    "    def test(self, epoch):\n",
    "        self.iteration(epoch, self.test_data, train=False)\n",
    "\n",
    "    def iteration(self, epoch, data_loader, train=True):\n",
    "        str_code = \"train\" if train else \"test\"\n",
    "\n",
    "        # Setting the tqdm progress bar\n",
    "        data_iter = tqdm.tqdm(enumerate(data_loader),\n",
    "                              desc=\"EP_%s:%d\" % (str_code, epoch),\n",
    "                              total=len(data_loader),\n",
    "                              bar_format=\"{l_bar}{r_bar}\")\n",
    "\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in data_iter:\n",
    "            # 0. batch_data will be sent into the device(GPU or cpu)\n",
    "            input_ids = data[\"input_ids\"].to(self.device)\n",
    "\n",
    "            # 1. forward the next_sentence_prediction and masked_lm model\n",
    "            mask_lm_output = self.model.forward(input_ids)\n",
    "\n",
    "            # 2-2. NLLLoss of predicting masked token word\n",
    "            loss = self.criterion(mask_lm_output.transpose(1, 2), data[\"input_ids\"])\n",
    "\n",
    "            # 3. backward and optimization only in train\n",
    "            if train:\n",
    "                self.optim_schedule.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optim_schedule.step_and_update_lr()\n",
    "\n",
    "            # next sentence prediction accuracy\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            post_fix = {\n",
    "                \"epoch\": epoch,\n",
    "                \"iter\": i,\n",
    "                \"avg_loss\": avg_loss / (i + 1),\n",
    "                \"loss\": loss.item()\n",
    "            }\n",
    "\n",
    "            if i % self.log_freq == 0:\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        print(\"EP%d_%s, avg_loss=\" % (epoch, str_code), avg_loss / len(data_iter))\n",
    "\n",
    "    def save(self, epoch, file_path=\"output/bert_trained.model\"):\n",
    "        \"\"\"\n",
    "        Saving the current BERT model on file_path\n",
    "\n",
    "        :param epoch: current epoch number\n",
    "        :param file_path: model output path which gonna be file_path+\"ep%d\" % epoch\n",
    "        :return: final_output_path\n",
    "        \"\"\"\n",
    "        output_path = file_path + \".ep%d\" % epoch\n",
    "        torch.save(self.bert.cpu(), output_path)\n",
    "        self.bert.to(self.device)\n",
    "        print(\"EP:%d Model Saved on:\" % epoch, output_path)\n",
    "        return output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c64545c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BERT model\n",
      "BERT model:  30522 256 8 8 0.1\n",
      "Embedding size:  30522 256 0.1\n",
      "Embedding dim:  256\n",
      "Creating BERT Trainer\n",
      "Device: cpu\n",
      "CUDA Available:  False\n",
      "Total Parameters: 21975866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EP_train:0:   0%|| 0/1152 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  torch.Size([64, 512, 256]) torch.Size([1, 512, 256])\n"
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "args = Namespace(\n",
    "    hidden=256,\n",
    "    batch_size=64,\n",
    "    layers=8,\n",
    "    attn_heads=8,\n",
    "    adam_weight_decay=0.01,\n",
    "    adam_beta1=0.9,\n",
    "    epochs=10,\n",
    "    log_freq=10,\n",
    "    adam_beta2=0.999,\n",
    "    cuda_devices=[0],\n",
    "    num_workers=4,\n",
    "    lr=1e-3,\n",
    "    with_cuda=True,\n",
    ")\n",
    "\n",
    "print(\"Building BERT model\")\n",
    "\n",
    "bert = BERT(\n",
    "    tokenizer.vocab_size,\n",
    "    hidden=args.hidden,\n",
    "    n_layers=args.layers,\n",
    "    attn_heads=args.attn_heads,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"], batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset[\"test\"], batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")\n",
    "\n",
    "print(\"Creating BERT Trainer\")\n",
    "trainer = BERTTrainer(\n",
    "    bert,\n",
    "    tokenizer.vocab_size,\n",
    "    train_dataloader=train_dataloader,\n",
    "    test_dataloader=test_dataloader,\n",
    "    lr=args.lr,\n",
    "    betas=(args.adam_beta1, args.adam_beta2),\n",
    "    weight_decay=args.adam_weight_decay,\n",
    "    log_freq=args.log_freq,\n",
    "    with_cuda=args.with_cuda,\n",
    "    cuda_devices=args.cuda_devices,\n",
    ")\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    trainer.train(epoch)\n",
    "    trainer.save(epoch, args.output_path)\n",
    "\n",
    "    if test_dataloader is not None:\n",
    "        trainer.test(epoch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
